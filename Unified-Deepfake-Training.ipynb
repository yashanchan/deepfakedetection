{"cells":[{"cell_type":"markdown","id":"7ee189aa","metadata":{"id":"7ee189aa"},"source":["# Definitive Deepfake Detector: A Two-Phase, Unified Training Workflow\n","\n","This notebook implements the final, most powerful version of our project.\n","\n","1.  **Workflow**: It uses an efficient **two-phase** process. We first pre-process all raw data (a slow, one-time step), then we train on this prepared data (a fast, repeatable step).\n","2.  **Architecture**: It uses the full **ResNet -> LSTM -> Transformer** model to analyze videos, and ResNet for images.\n","3.  **Training**: It trains **one single model** on a unified dataset of **both images and videos together**.\n","4.  **Output**: It produces a single, versatile model file: `best_unified_model.pth`."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zMIFcyQvodb","executionInfo":{"status":"ok","timestamp":1760278044464,"user_tz":-330,"elapsed":33458,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"4e68ac54-880c-4d34-9818-af5f65543e5f"},"id":"7zMIFcyQvodb","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"ec2a5826","metadata":{"id":"ec2a5826"},"source":["Before you start, place all your raw media files into a simple, unified folder structure. All real files (images and videos) go in `data/real`, and all fake files go in `data/fake`.\n","\n","**Required Input Structure:**"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/deepfake_train\n","!ls\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uSh5Jhuv--p","executionInfo":{"status":"ok","timestamp":1760278104164,"user_tz":-330,"elapsed":1369,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"deac379f-3d26-4de6-a646-cbf2d862d03f"},"id":"9uSh5Jhuv--p","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/deepfake_train\n","best_unified_model.pth\tdata_processed\t  Unified-Deepfake-Training.ipynb\n","data\t\t\trequirements.txt\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTVuU_sdwTQj","executionInfo":{"status":"ok","timestamp":1760278179278,"user_tz":-330,"elapsed":5366,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"207ee41b-4e25-45a7-f5bf-36fcf56d686f"},"id":"gTVuU_sdwTQj","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Flask in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (3.1.2)\n","Collecting Flask-Cors (from -r requirements.txt (line 2))\n","  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.12.0.88)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n","Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.5.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (11.3.0)\n","Requirement already satisfied: werkzeug in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.1.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.6.1)\n","Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.10.14)\n","Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask->-r requirements.txt (line 1)) (1.9.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from Flask->-r requirements.txt (line 1)) (8.3.0)\n","Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask->-r requirements.txt (line 1)) (2.2.0)\n","Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask->-r requirements.txt (line 1)) (3.1.6)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask->-r requirements.txt (line 1)) (3.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch->-r requirements.txt (line 5)) (2.32.4)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch->-r requirements.txt (line 5)) (0.23.0+cu126)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.16.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.6.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe->-r requirements.txt (line 10)) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe->-r requirements.txt (line 10)) (25.4.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe->-r requirements.txt (line 10)) (25.9.23)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe->-r requirements.txt (line 10)) (0.5.3)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe->-r requirements.txt (line 10)) (0.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe->-r requirements.txt (line 10)) (3.10.0)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe->-r requirements.txt (line 10)) (4.12.0.88)\n","Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe->-r requirements.txt (line 10)) (4.25.8)\n","Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe->-r requirements.txt (line 10)) (0.5.2)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe->-r requirements.txt (line 10)) (2.0.0)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe->-r requirements.txt (line 10)) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe->-r requirements.txt (line 10)) (3.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (2.9.0.post0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch->-r requirements.txt (line 5)) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch->-r requirements.txt (line 5)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch->-r requirements.txt (line 5)) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch->-r requirements.txt (line 5)) (2025.10.5)\n","Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->facenet-pytorch->-r requirements.txt (line 5)) (2.8.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (3.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (3.4.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe->-r requirements.txt (line 10)) (2.23)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe->-r requirements.txt (line 10)) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision->facenet-pytorch->-r requirements.txt (line 5)) (1.3.0)\n","Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n","Installing collected packages: Flask-Cors\n","Successfully installed Flask-Cors-6.0.1\n"]}]},{"cell_type":"code","execution_count":3,"id":"14f11473","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14f11473","executionInfo":{"status":"ok","timestamp":1760278151288,"user_tz":-330,"elapsed":31255,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"b9a40f8e-8290-4a81-ba27-fcb4274367fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mâœ… Libraries installed and imported.\n"]}],"source":["# This cell installs all necessary packages.\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q\n","!pip install opencv-python-headless numpy facenet-pytorch tqdm Pillow scikit-learn mediapipe -q\n","\n","import torch, torch.nn as nn, torchvision.models as models, os, math, cv2, numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from facenet_pytorch import MTCNN # Still needed for comparison/fallback if desired, but not used in new code\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import mediapipe as mp # NEW IMPORT\n","\n","print(\"âœ… Libraries installed and imported.\")"]},{"cell_type":"markdown","id":"b31a1ae4","metadata":{"id":"b31a1ae4"},"source":["## âš™ï¸ Phase 1: Pre-processing (Run This Cell Only Once)\n","\n","The following cell will perform the slow face detection and cropping on your entire raw dataset. It will save the clean, ready-to-use files in a new `data_processed` folder.\n","\n","**This will take a very long time, but you only need to run it once.**"]},{"cell_type":"code","execution_count":null,"id":"80a57a42","metadata":{"id":"80a57a42","outputId":"7df8a2e4-3aaa-4388-d75d-4971ac3df8c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running pre-processing with MediaPipe on device: cuda:0\n"]},{"name":"stderr","output_type":"stream","text":["Processing real files:   0%|          | 0/50 [00:00<?, ?it/s]c:\\anaconda\\envs\\deepfake-env\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n","  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n","Processing real files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:01<00:00,  4.83s/it]\n","Processing fake files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:10<00:00,  3.81s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","âœ… Pre-processing complete!\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# --- Configuration ---\n","SOURCE_DATA_PATH = './data'\n","PROCESSED_DATA_PATH = './data_processed'\n","FRAMES_PER_SECOND_TO_EXTRACT = 10 # Your requested frame rate\n","\n","# --- MediaPipe Initialization ---\n","mp_face_detection = mp.solutions.face_detection\n","face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n","\n","print(f'Running pre-processing with MediaPipe on device: {device}')\n","\n","def process_and_save_face(image_pil, dest_path):\n","    \"\"\"Uses MediaPipe to detect a face, crop it, resize, and save.\"\"\"\n","    image_np = np.array(image_pil)\n","    # MediaPipe expects BGR, so we convert from PIL's RGB\n","    results = face_detection.process(cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n","\n","    if not results.detections:\n","        return False # Indicate failure\n","\n","    # Get bounding box from the first detected face\n","    detection = results.detections[0]\n","    box = detection.location_data.relative_bounding_box\n","    h, w, _ = image_np.shape\n","\n","    xmin = int(box.xmin * w)\n","    ymin = int(box.ymin * h)\n","    width = int(box.width * w)\n","    height = int(box.height * h)\n","\n","    # Crop the image using PIL and save\n","    cropped_face = image_pil.crop((xmin, ymin, xmin + width, ymin + height))\n","    cropped_face.resize((224, 224)).save(dest_path)\n","    return True # Indicate success\n","\n","# This function processes a directory ('real' or 'fake')\n","def process_directory(class_name):\n","    source_class_dir = os.path.join(SOURCE_DATA_PATH, class_name)\n","    processed_class_dir = os.path.join(PROCESSED_DATA_PATH, class_name)\n","    os.makedirs(processed_class_dir, exist_ok=True)\n","\n","    if not os.path.isdir(source_class_dir): return\n","\n","    files_to_process = [f for f in os.listdir(source_class_dir)]\n","\n","    for file_name in tqdm(files_to_process, desc=f'Processing {class_name} files'):\n","        source_path = os.path.join(source_class_dir, file_name)\n","\n","        # --- Handle Videos (with 10 FPS logic) ---\n","        if any(file_name.lower().endswith(ext) for ext in ['.mp4', '.avi', '.mov']):\n","            video_id = os.path.splitext(file_name)[0]\n","            output_folder = os.path.join(processed_class_dir, video_id)\n","            if os.path.exists(output_folder): continue\n","\n","            try:\n","                cap = cv2.VideoCapture(source_path)\n","                fps = cap.get(cv2.CAP_PROP_FPS)\n","                if fps == 0: continue\n","\n","                frame_stride = int(fps / FRAMES_PER_SECOND_TO_EXTRACT)\n","                if frame_stride == 0: frame_stride = 1\n","\n","                os.makedirs(output_folder, exist_ok=True)\n","\n","                current_frame_index = 0\n","                saved_frame_count = 0\n","                while True:\n","                    ret, frame = cap.read()\n","                    if not ret: break\n","\n","                    if current_frame_index % frame_stride == 0:\n","                        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","                        save_path = os.path.join(output_folder, f'{saved_frame_count:03d}.jpg')\n","\n","                        # Use the MediaPipe function to process the frame\n","                        success = process_and_save_face(frame_pil, save_path)\n","                        if success:\n","                            saved_frame_count += 1\n","                    current_frame_index += 1\n","                cap.release()\n","            except Exception as e:\n","                print(f\"Failed to process video {file_name}: {e}\")\n","                continue\n","\n","        # --- Handle Images (with MediaPipe) ---\n","        elif any(file_name.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg']):\n","            dest_path = os.path.join(processed_class_dir, file_name)\n","            if os.path.exists(dest_path): continue\n","            try:\n","                img = Image.open(source_path).convert('RGB')\n","                process_and_save_face(img, dest_path)\n","            except Exception as e:\n","                print(f\"Failed to process image {file_name}: {e}\")\n","                continue\n","\n","# Run processing for both 'real' and 'fake' directories\n","process_directory('real')\n","process_directory('fake')\n","\n","print(\"\\nâœ… Pre-processing complete!\")"]},{"cell_type":"code","execution_count":32,"id":"9cbb34fd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cbb34fd","executionInfo":{"status":"ok","timestamp":1760285889312,"user_tz":-330,"elapsed":584,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"5be60d2c-5591-4fcb-abfe-d8fdefd6bf5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Training components defined.\n"]}],"source":["# --- 1. Final Dataset with Chunking Logic ---\n","class PreprocessedDataset(Dataset):\n","    def __init__(self, file_paths, labels, chunk_size=20):\n","        self.chunk_size = chunk_size\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","\n","        # --- NEW CHUNKING LOGIC ---\n","        # This new list will hold our final training samples (chunks and images)\n","        self.samples = []\n","\n","        print(\"Scanning and creating chunks from the dataset...\")\n","        for path, label in tqdm(zip(file_paths, labels), total=len(file_paths), desc=\"Chunking data\"):\n","            if os.path.isdir(path):\n","                # This is a video, so we create multiple chunks from it\n","                frame_files = sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg')])\n","                num_frames = len(frame_files)\n","\n","                # Create chunks of 'chunk_size' from the video frames\n","                for i in range(0, num_frames - chunk_size + 1, chunk_size):\n","                    sample_info = {\n","                        \"type\": \"video\",\n","                        \"path\": path,\n","                        \"start_index\": i,\n","                        \"label\": label\n","                    }\n","                    self.samples.append(sample_info)\n","            else:\n","                # This is an image, treat it as a single sample\n","                sample_info = {\n","                    \"type\": \"image\",\n","                    \"path\": path,\n","                    \"label\": label\n","                }\n","                self.samples.append(sample_info)\n","        print(f\"Expanded dataset from {len(file_paths)} files to {len(self.samples)} total training samples (chunks + images).\")\n","\n","    def __len__(self):\n","        # The length of the dataset is now the total number of chunks and images\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        sample_info = self.samples[idx]\n","        label = sample_info[\"label\"]\n","        path = sample_info[\"path\"]\n","\n","        if sample_info[\"type\"] == 'video':\n","            start_index = sample_info[\"start_index\"]\n","            frame_files = sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg')])\n","\n","            # Slice the list to get the correct chunk of frames\n","            chunk_files = frame_files[start_index : start_index + self.chunk_size]\n","\n","            try:\n","                frames = [Image.open(p).convert('RGB') for p in chunk_files]\n","                processed_data = torch.stack([self.transform(f) for f in frames])\n","                return processed_data, torch.tensor(label, dtype=torch.float32), 'video'\n","            except Exception:\n","                return None\n","        else: # type is 'image'\n","            try:\n","                img = Image.open(path).convert('RGB')\n","                processed_data = self.transform(img)\n","                return processed_data, torch.tensor(label, dtype=torch.float32), 'image'\n","            except Exception:\n","                return None\n","\n","# --- Collate Function (No changes needed) ---\n","def custom_collate_fn(batch):\n","    batch = [item for item in batch if item is not None]\n","    if not batch: return None\n","    images, videos, image_labels, video_labels = [], [], [], []\n","    for data, label, media_type in batch:\n","        if media_type == 'image': images.append(data); image_labels.append(label)\n","        else: videos.append(data); video_labels.append(label)\n","    processed_batch = {}\n","    if images: processed_batch['image'] = (torch.stack(images), torch.stack(image_labels))\n","    if videos: processed_batch['video'] = (torch.stack(videos), torch.stack(video_labels))\n","    return processed_batch\n","\n","# --- Final Model Definition (No changes needed) ---\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout, pe = nn.Dropout(p=dropout), torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2], pe[:, 1::2] = torch.sin(position * div_term), torch.cos(position * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0).transpose(0, 1))\n","    def forward(self, x): return self.dropout(x + self.pe[:x.size(0), :])\n","\n","class ResNetLSTMTransformer(nn.Module):\n","    def __init__(self, num_classes=1, d_model=512, nhead=8, num_encoder_layers=3, dim_feedforward=1024, freeze_resnet=True):\n","        super(ResNetLSTMTransformer, self).__init__()\n","        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n","        if freeze_resnet:\n","            for param in resnet.parameters(): param.requires_grad = False\n","        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n","        self.lstm = nn.LSTM(input_size=2048, hidden_size=d_model, num_layers=2, batch_first=True, bidirectional=True)\n","        self.lstm_output_layer = nn.Linear(d_model * 2, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model=d_model)\n","        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_encoder_layers)\n","        self.classifier_video = nn.Sequential(nn.Linear(d_model, 256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, num_classes))\n","        self.classifier_image = nn.Sequential(nn.Linear(2048, 256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, num_classes))\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        is_video = x.dim() == 5\n","        if is_video:\n","            batch_size, seq_length, c, h, w = x.shape\n","            x = x.view(batch_size * seq_length, c, h, w)\n","        features = self.resnet_features(x).view(x.size(0), -1)\n","        if is_video:\n","            features = features.view(batch_size, seq_length, -1)\n","            features = self.lstm_output_layer(self.lstm(features)[0])\n","            features = self.pos_encoder(features)\n","            output = self.classifier_video(self.transformer_encoder(features).mean(dim=1))\n","        else:\n","            output = self.classifier_image(features)\n","        return output\n","\n","print(\"âœ… Training components defined.\")"]},{"cell_type":"code","execution_count":36,"id":"d66a6284","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d66a6284","executionInfo":{"status":"ok","timestamp":1760287906736,"user_tz":-330,"elapsed":317,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"e741c95a-b324-456c-f574-f1139d13275f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda:0\n","Found 100 total pre-processed items to train on together.\n","Scanning and creating chunks from the dataset...\n"]},{"output_type":"stream","name":"stderr","text":["Chunking data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:00<00:00, 346.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Expanded dataset from 80 files to 615 total training samples (chunks + images).\n","Scanning and creating chunks from the dataset...\n"]},{"output_type":"stream","name":"stderr","text":["Chunking data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 301.59it/s]"]},{"output_type":"stream","name":"stdout","text":["Expanded dataset from 20 files to 188 total training samples (chunks + images).\n","âœ… Datasets and DataLoaders created successfully.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# --- âš™ï¸ TRAINING CONFIGURATION ---\n","DATA_PATH = '/content/drive/MyDrive/deepfake_train/data_processed'\n","EPOCHS = 10\n","BATCH_SIZE = 16\n","LEARNING_RATE = 1e-7\n","NUM_FRAMES = 20\n","MODEL_SAVE_PATH = '/content/drive/MyDrive/deepfake_train/best_unified_model.pth'\n","\n","# --- Setup Device ---\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# --- File Collection (from processed data) ---\n","real_path, fake_path = os.path.join(DATA_PATH, 'real'), os.path.join(DATA_PATH, 'fake')\n","if not os.path.exists(real_path) or not os.path.exists(fake_path):\n","    print(f\"âŒ CRITICAL ERROR: Pre-processed data not found. Please run the Phase 1 cell first.\")\n","else:\n","    real_files, fake_files = [os.path.join(real_path, f) for f in os.listdir(real_path)], [os.path.join(fake_path, f) for f in os.listdir(fake_path)]\n","    files, labels = real_files + fake_files, [0] * len(real_files) + [1] * len(fake_files)\n","\n","    if not files:\n","        print(\"âŒ CRITICAL ERROR: No pre-processed files found. Please run the Phase 1 cell.\")\n","    else:\n","        print(f\"Found {len(files)} total pre-processed items to train on together.\")\n","        train_files, val_files, train_labels, val_labels = train_test_split(files, labels, test_size=0.2, stratify=labels, random_state=42)\n","\n","        train_dataset, val_dataset = PreprocessedDataset(train_files, train_labels, NUM_FRAMES), PreprocessedDataset(val_files, val_labels, NUM_FRAMES)\n","        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n","        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n","        print(\"âœ… Datasets and DataLoaders created successfully.\")"]},{"cell_type":"code","source":["# === RESUME FROM CHECKPOINT (if available) ===\n","import os\n","import torch\n","\n","# device variable is set in Cell 6 already; ensure it's available\n","try:\n","    device\n","except NameError:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(\"Device for training:\", device)\n","print(\"Looking for checkpoint at:\", MODEL_SAVE_PATH)\n","\n","start_epoch = 0\n","checkpoint = None\n","if os.path.exists(MODEL_SAVE_PATH):\n","    print(\"Found checkpoint. Loading...\")\n","    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n","    # checkpoint might be a state_dict or a dict with 'state_dict' and other keys\n","else:\n","    print(\"No checkpoint found; training from scratch.\")\n","\n","# Instantiate model architecture (same code as later in the notebook)\n","model = ResNetLSTMTransformer().to(device)\n","\n","# Load weights if checkpoint contains state_dict\n","if checkpoint is not None:\n","    try:\n","        # Most common case: saved state_dict directly\n","        model.load_state_dict(checkpoint)\n","        print(\"âœ… Loaded checkpoint as state_dict.\")\n","    except Exception as e1:\n","        try:\n","            # Another common pattern: checkpoint is a dict with 'state_dict'\n","            model.load_state_dict(checkpoint.get('state_dict', checkpoint))\n","            print(\"âœ… Loaded checkpoint['state_dict'].\")\n","        except Exception as e2:\n","            print(\"âš ï¸ Could not load checkpoint automatically:\", e2)\n","            raise\n","\n","# Prepare loss and optimizer as in the original training cell\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# If checkpoint also stores optimizer and epoch, restore them\n","if checkpoint is not None and isinstance(checkpoint, dict):\n","    if 'optimizer' in checkpoint:\n","        try:\n","            optimizer.load_state_dict(checkpoint['optimizer'])\n","            print(\"âœ… Loaded optimizer state from checkpoint.\")\n","        except Exception as e:\n","            print(\"âš ï¸ Could not load optimizer state:\", e)\n","    if 'epoch' in checkpoint:\n","        start_epoch = checkpoint['epoch'] + 1\n","        print(f\"Will resume from epoch {start_epoch}\")\n","\n","print(\"Model and optimizer ready. start_epoch =\", start_epoch)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tYfFLxxPxoEc","executionInfo":{"status":"ok","timestamp":1760287910451,"user_tz":-330,"elapsed":1150,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"6437c77a-d203-4c8e-8dd8-2ba2c830eca8"},"id":"tYfFLxxPxoEc","execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Device for training: cuda:0\n","Looking for checkpoint at: /content/drive/MyDrive/deepfake_train/best_unified_model.pth\n","Found checkpoint. Loading...\n","âœ… Loaded checkpoint as state_dict.\n","Model and optimizer ready. start_epoch = 0\n"]}]},{"cell_type":"code","execution_count":38,"id":"d09853db","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d09853db","executionInfo":{"status":"ok","timestamp":1760289382876,"user_tz":-330,"elapsed":7,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"4f32bafc-85a7-4abb-b436-d92f3ca568f9"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:51<00:00,  2.86s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.82s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/10 -> Train Loss: 0.6954, Acc: 0.5350 | Val Loss: 0.6833, Acc: 0.5798\n","âœ… Saved new best model to /content/drive/MyDrive/deepfake_train/best_unified_model.pth\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:56<00:00,  2.98s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:34<00:00,  2.83s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 2/10 -> Train Loss: 0.6949, Acc: 0.5220 | Val Loss: 0.6834, Acc: 0.5798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:52<00:00,  2.87s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.77s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 3/10 -> Train Loss: 0.6991, Acc: 0.5138 | Val Loss: 0.6834, Acc: 0.5798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:52<00:00,  2.88s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.75s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 4/10 -> Train Loss: 0.6983, Acc: 0.5187 | Val Loss: 0.6835, Acc: 0.5798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:51<00:00,  2.86s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.77s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 5/10 -> Train Loss: 0.7041, Acc: 0.5073 | Val Loss: 0.6836, Acc: 0.5798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:52<00:00,  2.88s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.80s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 6/10 -> Train Loss: 0.6988, Acc: 0.5171 | Val Loss: 0.6836, Acc: 0.5798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:52<00:00,  2.88s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.80s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 7/10 -> Train Loss: 0.7010, Acc: 0.5317 | Val Loss: 0.6838, Acc: 0.5798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:51<00:00,  2.86s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.81s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 8/10 -> Train Loss: 0.7042, Acc: 0.5187 | Val Loss: 0.6838, Acc: 0.5798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:52<00:00,  2.89s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.83s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 9/10 -> Train Loss: 0.7036, Acc: 0.5268 | Val Loss: 0.6838, Acc: 0.5798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [01:53<00:00,  2.90s/it]\n","Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:33<00:00,  2.77s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 10/10 -> Train Loss: 0.6921, Acc: 0.5317 | Val Loss: 0.6838, Acc: 0.5798\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# --- ğŸ§  TRAINING LOOP ---\n","\n","def train_one_epoch(model, dataloader, criterion, optimizer, device):\n","    model.train()\n","    total_loss, total_acc, total_samples = 0.0, 0, 0\n","    for batch in tqdm(dataloader, desc=\"Training\"):\n","        if batch is None: continue\n","        for media_type, (inputs, labels) in batch.items():\n","            inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item() * inputs.size(0)\n","            total_acc += ((torch.sigmoid(outputs) > 0.5) == labels).sum().item()\n","            total_samples += labels.size(0)\n","    return total_loss/total_samples if total_samples > 0 else 0, total_acc/total_samples if total_samples > 0 else 0\n","\n","def validate_one_epoch(model, dataloader, criterion, device):\n","    model.eval()\n","    total_loss, total_acc, total_samples = 0.0, 0, 0\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Validating\"):\n","            if batch is None: continue\n","            for media_type, (inputs, labels) in batch.items():\n","                inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                total_loss += loss.item() * inputs.size(0)\n","                total_acc += ((torch.sigmoid(outputs) > 0.5) == labels).sum().item()\n","                total_samples += labels.size(0)\n","    return total_loss/total_samples if total_samples > 0 else 0, total_acc/total_samples if total_samples > 0 else 0\n","\n","model = ResNetLSTMTransformer().to(device)\n","criterion, optimizer = nn.BCEWithLogitsLoss(), torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","best_val_loss = float('inf')\n","\n","for epoch in range(EPOCHS):\n","    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n","    print(f\"Epoch {epoch+1}/{EPOCHS} -> Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n","    if val_loss > 0 and val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n","        print(f\"âœ… Saved new best model to {MODEL_SAVE_PATH}\")"]},{"cell_type":"code","source":["# Restarting kernel recommended after many tries. Then run:\n","import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()\n","!nvidia-smi\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0RlCOHI3F8G","executionInfo":{"status":"ok","timestamp":1760279950755,"user_tz":-330,"elapsed":179,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"90795980-2917-46e5-abcf-2be6d4648e91"},"id":"y0RlCOHI3F8G","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Oct 12 14:39:11 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   76C    P0             34W /   70W |    8708MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":5,"id":"378ed210","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"378ed210","executionInfo":{"status":"ok","timestamp":1760510555312,"user_tz":-330,"elapsed":10,"user":{"displayName":"preetham Im","userId":"07476322355613146172"}},"outputId":"3850453e-d417-436a-91e2-c78231a71cc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["âŒ Error: Check that your model path ('/content/drive/MyDrive/deepfake_train/best_unified_model.pth') and file path ('/content/drive/MyDrive/deepfake_train/we.jpg') are correct.\n"]}],"source":["# --- ğŸš€ PREDICTION ---\n","import os # Import the os module\n","\n","def predict_raw_file(file_path, model_path):\n","    pred_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {pred_device}\")\n","\n","    mtcnn = MTCNN(image_size=224, margin=20, device=pred_device, keep_all=False, post_process=False)\n","    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n","\n","    pred_model = ResNetLSTMTransformer().to(pred_device)\n","    pred_model.load_state_dict(torch.load(model_path, map_location=pred_device))\n","    pred_model.eval()\n","\n","    tensor_data, media_type = None, ''\n","    is_video = any(file_path.lower().endswith(ext) for ext in ['.mp4', '.avi', '.mov'])\n","\n","    if is_video:\n","        media_type = 'video'\n","        cap = cv2.VideoCapture(file_path)\n","        frames = []\n","        if cap.isOpened():\n","            total_f = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","            indices = np.linspace(0, total_f - 1, 20, dtype=int)\n","            for i in sorted(indices):\n","                cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","                ret, frame = cap.read()\n","                if ret: frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n","            cap.release()\n","        if frames:\n","            faces = mtcnn(frames)\n","            if faces is not None and len(faces) > 0:\n","                face_frames = [transforms.ToPILImage()((f+1)/2) for f in faces if f is not None]\n","                if len(face_frames) < 20: face_frames.extend([face_frames[-1]]*(20-len(face_frames)))\n","                tensor_data = torch.stack([transform(f) for f in face_frames])\n","    else:\n","        media_type = 'image'\n","        try:\n","            face = mtcnn(Image.open(file_path).convert('RGB'))\n","            if face is not None: tensor_data = transform((face + 1) / 2)\n","        except Exception: tensor_data = None\n","\n","    if tensor_data is None or (is_video and tensor_data.size(0) == 0):\n","        print(\"âŒ Error: Could not detect a face in the provided file.\")\n","        return\n","\n","    with torch.no_grad():\n","        if is_video:\n","             # Add a batch dimension for video input\n","            output = pred_model(tensor_data.unsqueeze(0).to(pred_device))\n","        else:\n","            output = pred_model(tensor_data.unsqueeze(0).to(pred_device))\n","        prob_fake = torch.sigmoid(output).item()\n","\n","    UNCERTAIN_LOW, UNCERTAIN_HIGH = 0.40, 0.60\n","    if UNCERTAIN_LOW < prob_fake < UNCERTAIN_HIGH:\n","        prediction, confidence_str = \"Unable to Predict\", \"\"\n","    elif prob_fake >= UNCERTAIN_HIGH:\n","        prediction, confidence_str = \"FAKE\", f\"({prob_fake * 100:.2f}% confidence)\"\n","    else:\n","        prediction, confidence_str = \"REAL\", f\"({(1 - prob_fake) * 100:.2f}% confidence)\"\n","\n","    print(f\"\\n--- Prediction Result for {media_type} ---\")\n","    print(f\"File: {os.path.basename(file_path)}\")\n","    print(f\"Result: {prediction} {confidence_str}\")\n","\n","# --- âš ï¸ USAGE ---\n","test_model_path = '/content/drive/MyDrive/deepfake_train/best_unified_model.pth' # Assumes model is saved here\n","test_file_path = '/content/drive/MyDrive/deepfake_train/we.jpg' # <-- CHANGE this to the path of your test file (image or video)\n","\n","if os.path.exists(test_model_path) and os.path.exists(test_file_path):\n","    predict_raw_file(test_file_path, test_model_path)\n","else:\n","    print(f\"âŒ Error: Check that your model path ('{test_model_path}') and file path ('{test_file_path}') are correct.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}